# ğŸ¢ Data Warehouse - ML System Design Note

> **An enterprise store for structured and semi-structured data from many sources, designed for analytics, reporting, and business intelligence.**

![Data Warehouse Architecture](assets/data-warehouse.png)

---

## ğŸ¯ When to use

âœ… **Many disparate sources** - need a single source of truth  
âœ… **Ad-hoc SQL queries** - dashboards and governed data access  
âœ… **Historical analysis** - trend analysis, KPI tracking, decision support  
âœ… **Strong SLAs** - freshness, cost control, and security requirements  

---

## ğŸ”„ Architecture Flow

```
ğŸ“Š Sources â†’ ğŸ“¥ Ingestion â†’ ğŸ”„ Staging â†’ âš¡ Transform â†’ ğŸ¢ Data Warehouse â†’ ğŸ“ˆ Visualization
```

Data flows from various sources through ingestion pipelines (batch or streaming), gets staged, transformed via ETL/ELT processes, stored in a structured warehouse with facts and dimensions, then consumed by BI tools.

---

## âš™ï¸ Core Design Decisions

### ğŸ”€ ETL vs ELT
| Approach | When to Use | Benefits |
|----------|-------------|----------|
| **ELT** | Modern, scalable systems | Load first, transform in-warehouse. More flexible |
| **ETL** | Strict governance needs | Transform before loading. Better control |

### ğŸ“ Data Modeling
- **â­ Star Schema** - Denormalized, faster for BI queries
- **â„ï¸ Snowflake Schema** - Normalized, saves storage but more complex joins
- **ğŸ”„ Slowly Changing Dimensions** - Track history with SCD Type-1 (overwrite) or Type-2 (versioning)

### â±ï¸ Data Freshness
- **ğŸ”„ Batch Processing** - Hourly/daily updates, simpler to manage
- **ğŸŒŠ Stream Processing** - Near real-time, handles late arrivals and duplicates
- **âš¡ Watermarks** - Plan for out-of-order data

### ğŸš€ Performance Optimization
- **ğŸ“‚ Partitioning** - By `event_date` or `load_date` for time-based queries
- **ğŸ¯ Clustering** - By high-cardinality columns like `customer_id`, `country`
- **ğŸ’¾ Storage Format** - Columnar formats (Parquet, ORC) with compression
- **ğŸ“Š Materialized Views** - Pre-compute heavy aggregations

---

## ğŸ›¡ï¸ Data Quality & Governance

### ğŸ” Testing & Contracts
- âœ… Define data contracts between teams
- âœ… Implement data quality tests (dbt tests, Great Expectations)
- âœ… Track data lineage for impact analysis

### ğŸ”’ Security & Compliance
- ğŸ” **PII Handling** - Masking or tokenization
- ğŸ‘¥ **Access Control** - Role-based permissions
- ğŸ›¡ï¸ **Encryption** - At rest and in transit

### ğŸ“š Metadata Management
- ğŸ“‹ Document data owners and SLAs
- ğŸ“Š Track freshness and cost metrics
- ğŸ“– Maintain business glossary

---

## ğŸ’° Performance & Cost Optimization

### ğŸ” Query Optimization
- âœ… Use denormalized read models for BI workloads
- âœ… Pre-aggregate frequently queried data
- âœ… Always include partition filters in queries
- âŒ Avoid `SELECT *` in production

### ğŸ›ï¸ Resource Management
- ğŸ’¾ Implement result caching where possible
- â° Use time travel features sparingly
- ğŸ§¹ Schedule automatic maintenance (vacuum, optimize)
- ğŸ“Š Set up cost monitoring and alerts

---

## ğŸ¤– ML-Specific Considerations

### ğŸ“Š Training Data Integrity
- â° **Point-in-time correctness** - Avoid data leakage
- ğŸ“¸ **Snapshots** - Raw and staging tables for reproducibility
- ğŸ“ **Documentation** - Feature lineage and definitions

### ğŸ”„ Training-Serving Alignment
- ğŸ¯ Keep training and serving schemas aligned
- ğŸ“Š Monitor for training-serving skew
- ğŸª Consider feeding a feature store from the warehouse

---

## ğŸ› ï¸ Technology Stack Options

### â˜ï¸ Cloud Data Warehouses
- **BigQuery** (Google Cloud)
- **Snowflake** (Multi-cloud)
- **Redshift** (AWS)

### ğŸ  Lakehouse Architecture
- **Storage**: S3, GCS, ADLS
- **Table Formats**: Delta Lake, Iceberg, Hudi
- **Query Engines**: Trino, Presto, Athena

### ğŸ”§ Supporting Tools
| Category | Options |
|----------|---------|
| **Orchestration** | Airflow, Dagster |
| **Transformation** | dbt, Spark |
| **Ingestion** | Fivetran, Airbyte, Kafka |
| **BI Tools** | Looker, Tableau, Metabase, Superset |

---

## âœ… Implementation Checklist

**Before going to production:**

- [ ] ğŸ“‹ Define data sources, SLAs, and contracts
- [ ] ğŸ—ï¸ Build staging to dimensional model pipeline
- [ ] ğŸ“Š Choose partitioning and clustering strategy
- [ ] ğŸ” Implement data quality tests and lineage tracking
- [ ] ğŸ”’ Set up PII policies and access controls
- [ ] ğŸ’° Configure cost guardrails and monitoring
- [ ] ğŸ”„ Create backfill and recovery procedures

---

## ğŸ·ï¸ Naming Conventions

Use consistent layer prefixes:
- `raw_*` - Source data
- `stg_*` - Staging/cleaning
- `dim_*` - Dimension tables
- `fct_*` - Fact tables

---

## ğŸ’» Example Query Pattern

```sql
-- ğŸ¯ Partition-pruned revenue analysis
SELECT 
    d.customer_id, 
    SUM(f.net_revenue) AS total_revenue
FROM fct_orders f
JOIN dim_customers d ON d.customer_key = f.customer_key
WHERE f.event_date BETWEEN DATE '2025-08-01' AND DATE '2025-08-31'
  AND d.country = 'Turkey'
GROUP BY d.customer_id
ORDER BY total_revenue DESC;
```

---

## ğŸ¯ Key Takeaways

> **Data warehouses excel at structured analytics and reporting with strong governance.** They provide a reliable foundation for business intelligence and can support ML workflows when designed with proper time-based partitioning and data quality controls. 
> 
> **Choose your architecture based on scale, budget, and team expertise.**

---

## ğŸ“š More Resources

**â­ Star this repo if you found it helpful!**

*More ML System Design notes coming soon...*
